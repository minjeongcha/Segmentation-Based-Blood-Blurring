{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cd191f-5d50-4e23-94e7-f482288f3a5b",
   "metadata": {},
   "source": [
    "## <B>Normalize Gaze Coordinate Data</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df881d-d4ec-4ad0-80ed-74b13bb53532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries you need\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "name = \"37\"\n",
    "\n",
    "movies = [\"기생충1\", \"기생충2\", \"낭닥1\", \"낭닥2\", \"코1\", \"코2\",  \"마녀1\", \"마녀2\", \"레버1\", \"레버2\"]\n",
    "\n",
    "for movie in movies:\n",
    "    # Importing data files\n",
    "    data = pd.read_csv(f'{name} {movie}.csv')\n",
    "    \n",
    "    # Check column names and remove spaces\n",
    "    data.columns = data.columns.str.strip()  # Remove unnecessary spaces in column names\n",
    "    \n",
    "    # Check the X and Y columns\n",
    "    print(\"원본 데이터:\")\n",
    "    print(data.head())\n",
    "    \n",
    "    # Normalize using Min-Max Scaler (range: -1 to 1)\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    \n",
    "    # Normalize X and Y\n",
    "    data[['X', 'Y']] = scaler.fit_transform(data[['X', 'Y']])\n",
    "    \n",
    "    # Check the results\n",
    "    print(\"\\nNormalized data:\")\n",
    "    print(data.head())\n",
    "    \n",
    "    # Save the normalized data as a new CSV file\n",
    "    data.to_csv(f'{name} {movie}(정규화).csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416014e3-32ef-4440-b4c5-c57a0136fc5d",
   "metadata": {},
   "source": [
    "## <B>Presence of eyes in blurred areas of experimental footage O/X</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3455f0-cdb6-4621-8ecb-118bda301547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Set to ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define OX analytics and integration processing functions\n",
    "def process_and_analyze(name, movie, box_file):\n",
    "    try:\n",
    "        # Read data\n",
    "        points_df = pd.read_csv(f'{name} {movie}(정규화).csv')  # Normalized point data\n",
    "        boxes_df = pd.read_csv(box_file)  # Box data\n",
    "\n",
    "        # Convert and reset timestamps\n",
    "        points_df['Timestamp (Current Time)'] = pd.to_datetime(points_df['Timestamp (Current Time)'], errors='coerce')\n",
    "        points_df.dropna(subset=['Timestamp (Current Time)'], inplace=True)  # Delete NaT\n",
    "\n",
    "        # Setting the base time and converting to seconds\n",
    "        base_time = points_df['Timestamp (Current Time)'].iloc[0]\n",
    "        points_df['Time (seconds)'] = (points_df['Timestamp (Current Time)'] - base_time).dt.total_seconds()\n",
    "\n",
    "        # Rounding and deduplicating time values\n",
    "        points_df['Time (seconds)'] = points_df['Time (seconds)'].round(1)\n",
    "        boxes_df['Time (seconds)'] = boxes_df['Time (seconds)'].round(1)\n",
    "        points_df = points_df.drop_duplicates(subset='Time (seconds)', keep='first')\n",
    "        boxes_df = boxes_df.drop_duplicates(subset='Time (seconds)', keep='first')\n",
    "\n",
    "        # Merge by time\n",
    "        merged_df = pd.merge(boxes_df, points_df, how='left', on='Time (seconds)')\n",
    "        merged_df.to_csv(f'{name}_{movie}_merged.csv', index=False)  # Save the merge results\n",
    "\n",
    "        # Calculate OX results\n",
    "        def point_box_with_margin(row):\n",
    "            expanded_x1 = row['x1'] - 0.25\n",
    "            expanded_y1 = row['y1'] - 0.25\n",
    "            expanded_x2 = row['x2'] + 0.25\n",
    "            expanded_y2 = row['y2'] + 0.25\n",
    "            return \"O\" if expanded_x1 <= row['X'] <= expanded_x2 and expanded_y1 <= row['Y'] <= expanded_y2 else \"X\"\n",
    "\n",
    "        merged_df['Result'] = merged_df.apply(point_box_with_margin, axis=1)\n",
    "\n",
    "        # Calculate and output the O ratio\n",
    "        o_count = merged_df['Result'].value_counts().get('O', 0)\n",
    "        total_count = len(merged_df)\n",
    "        o_ratio = o_count / total_count if total_count > 0 else 0\n",
    "\n",
    "        print(f'\"{movie}\" Analysis results:')\n",
    "        print(f' Number of \"O\": {o_count}')\n",
    "        print(f' Percentage of \"O\": {o_ratio:.2%}')\n",
    "\n",
    "        # Save Final Result\n",
    "        merged_df.to_csv(f'{name} {movie}_result.csv', index=False)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Example run\n",
    "name = \"37\"\n",
    "movie_box_map = {\n",
    "    \"기생충1\": \"영화1 보정.csv\",\n",
    "    \"기생충2\": \"영화1 보정.csv\",\n",
    "    \"낭닥1\": \"영화2 보정.csv\",\n",
    "    \"낭닥2\": \"영화2 보정.csv\",\n",
    "    \"코1\": \"영화3 보정.csv\",\n",
    "    \"코2\": \"영화3 보정.csv\",\n",
    "    \"마녀1\": \"영화4 보정.csv\",\n",
    "    \"마녀2\": \"영화4 보정.csv\",\n",
    "    \"레버1\": \"영화5 보정.csv\",\n",
    "    \"레버2\": \"영화5 보정.csv\",\n",
    "}\n",
    "\n",
    "for movie, box_file in movie_box_map.items():\n",
    "    process_and_analyze(name, movie, box_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
